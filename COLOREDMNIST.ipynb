{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "import sklearn\n",
    "\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import ref \n",
    "\n",
    "EPOCHSTEPS = 1000000\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#get embeddings \n",
    "\n",
    "def load_word_vectors(fname):\n",
    "    model = KeyedVectors.load_word2vec_format(fname, limit=200000, binary=False)\n",
    "    vecs = model.vectors\n",
    "    words = list(model.vocab.keys())\n",
    "    return model, vecs, words\n",
    "\n",
    "word2vec, vecs, words = load_word_vectors(\"crawl-300d-2M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_proc(txt, stopwords=[], lem=None):\n",
    "    words = txt.split(\" \")\n",
    "    words = {re.sub(r'\\W+', '', w).lower() for w in words \\\n",
    "             if re.sub(r'\\W+', '', w).lower() not in stopwords}\n",
    "    if lem is not None: \n",
    "        words = {lem.lemmatize(w) for w in words}\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicityDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd dataframe): The pd dataframe with (uid, tox_label, text)\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.dataset = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset.loc[idx, 'comment_text']\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        target = self.dataset.loc[idx, 'toxicity']\n",
    "        \n",
    "        return {'x':sample, 'y':np.expand_dims(target.values, axis=1)} #Fix this without hack\n",
    "    \n",
    "class GetEmbedding(object):\n",
    "    \"\"\"Given a sentence of text, generate sentence embedding\n",
    "\n",
    "    Args:\n",
    "        model: word embedding model, dictionary of wrods -> embeds\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, stopwords=[]):\n",
    "        self.model = model\n",
    "        self.stopwords = stopwords\n",
    "        self.unknown_embed = np.zeros(300)  #NOTE - this may not be same for all WEs \n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        ''':param sample: pd.Series'''\n",
    "        if type(sample) == str:\n",
    "            words = sample.split(\" \")\n",
    "            words = [w for w in words if w.lower() not in self.stopwords]\n",
    "            sent_embedding = np.sum([self.model[w] if w in self.model else self.unknown_embed for w in words], axis = 0)    \n",
    "        elif type(sample) == pd.Series:\n",
    "            sent_embedding = np.zeros((len(sample), 300))\n",
    "            for i, txt in enumerate(sample):\n",
    "                words = txt.split(\" \")\n",
    "                words = [w for w in words if w.lower() not in self.stopwords]\n",
    "                sent_embedding[i, :] = np.sum([self.model[w] if w in self.model else self.unknown_embed for w in words], axis = 0)   \n",
    "        \n",
    "        return sent_embedding\n",
    "    \n",
    "class GetBOW(object):\n",
    "    \"\"\"Given a sentence of text, generate BOW rep\n",
    "\n",
    "    Args:\n",
    "        vocab: dictionary, word-->index in array (assume contigious) \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, lem=None, stopwords=[]):\n",
    "        self.vocab = vocab\n",
    "        self.stopwords = stopwords\n",
    "        self.lem = lem\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        ''':param sample: str or pd.Series'''\n",
    "        def get_rep(txt):\n",
    "            rep = np.zeros(len(self.vocab))\n",
    "            words = sent_proc(txt, stopwords=self.stopwords, lem=self.lem)\n",
    "            for w in words:\n",
    "                try:\n",
    "                    rep[self.vocab[w]] = 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            return rep\n",
    "        \n",
    "        if type(sample) == str:\n",
    "            bow_embed = get_rep(sample)\n",
    "        elif type(sample) == pd.Series:\n",
    "            bow_embed = np.zeros((len(sample), len(self.vocab)))\n",
    "            for i, txt in enumerate(sample):\n",
    "                bow_embed[i, :] = get_rep(txt)\n",
    "                \n",
    "        return bow_embed\n",
    "\n",
    "#Data Functions\n",
    "def generate_dataset(d, elg, t=None):\n",
    "    full = d[['id', 'toxicity', 'comment_text']]\n",
    "    full = full[elg]\n",
    "    \n",
    "    #convert to pytorch formatting\n",
    "    full = ToxicityDataset(full, transform=t)\n",
    "    return full\n",
    "\n",
    "def generate_dataloader(d, elg, nbatch=1, t=None):\n",
    "    full = generate_dataset(d, elg, t=t)\n",
    "    full = DataLoader(full, ceil(len(full)/nbatch), shuffle=False)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('all_data.csv')\n",
    "print(full_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresh = 0.2\n",
    "#Data Cleaning \n",
    "full_data['LGTBQ'] = full_data[['homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation']].max(axis=1)\n",
    "\n",
    "full_partition = full_data[(full_data['LGTBQ'] > 0)]\n",
    "toxic, non_toxic = full_partition[full_data['toxicity'] >= thresh].sample(frac=1).reset_index(drop=True), \\\n",
    "                        full_partition[full_data['toxicity'] < thresh].sample(frac=1).reset_index(drop=True)\n",
    "toxic['toxicity'], non_toxic['toxicity'] = toxic['toxicity'].apply((lambda x: 1 if x > thresh else 0)), \\\n",
    "                    non_toxic['toxicity'].apply((lambda x: 1 if x > thresh else 0))\n",
    "\n",
    "totals = {'nt':len(non_toxic), 't':len(toxic)}\n",
    "env_splits = np.array([[.1, 0.9], [0.2, 0.8], [0.9, 0.1]]) \n",
    "weights = {'nt':env_splits.mean(axis=0)[0], 't':env_splits.mean(axis=0)[1]}\n",
    "\n",
    "#Adjust so that desired env splits possible\n",
    "\n",
    "if float(totals['t']/(totals['t'] + totals['nt'])) >= weights['t']:  #see who has the bigger proportion \n",
    "    ns = int(totals['nt']/weights['nt'] - totals['nt'])   #     int((len(full_partition) - weights['nt']*totals['nt'])/weights['t'])\n",
    "    toxic = toxic.sample(n=ns)\n",
    "else:\n",
    "    ns = int(totals['t']/weights['t'] - totals['t']) \n",
    "    non_toxic = non_toxic.sample(n=ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition env splits\n",
    "nenvs = env_splits.shape[0]\n",
    "e_props = env_splits/env_splits.sum(axis=0) #proprotion of vector in each env\n",
    "\n",
    "env_partitions = []  #Note - last env is the test env\n",
    "for i in range(nenvs):  #Note - tehre might be an error here that excludes  single sample from diff envs \n",
    "    #Get both componenets of envs \n",
    "    past_ind = int(np.array(e_props[:i, 0]).sum() * len(non_toxic))    \n",
    "    pres_ind = int(np.array(e_props[:(i+1), 0]).sum() * len(non_toxic))\n",
    "    nt = non_toxic.iloc[past_ind:pres_ind]\n",
    "\n",
    "    past_ind = int(np.array(e_props[:i, 1]).sum() * len(toxic))    \n",
    "    pres_ind = int(np.array(e_props[:(i+1), 1]).sum() * len(toxic))\n",
    "    t = toxic.iloc[past_ind:pres_ind]\n",
    "    \n",
    "    #Make full env \n",
    "    env_partitions.append(pd.concat([nt, t], ignore_index=True).sample(frac=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = GetEmbedding(word2vec, stopwords=STOPWORDS)\n",
    "train_partition = ToxicityDataset(pd.concat([e for e in env_partitions[:-1]], \\\n",
    "                                            ignore_index=True)[['id', 'toxicity', 'comment_text']], transform=t)[:]  \n",
    "test_partition = ToxicityDataset(env_partitions[-1][['id', 'toxicity', 'comment_text']], transform=t)[:]\n",
    "\n",
    "print(train_partition['x'].shape, test_partition['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(fit_intercept = True, penalty = 'l2').fit(train_partition['x'], train_partition['y'])\n",
    "print('train score: {}'.format(model.score(train_partition['x'], train_partition['y'])))\n",
    "print('test score: {}'.format(model.score(test_partition['x'], test_partition['y'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRM Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ref \n",
    "t = GetEmbedding(word2vec, stopwords=STOPWORDS)\n",
    "train_envs = [ToxicityDataset(e[['id', 'toxicity', 'comment_text']], transform=t)[:] for e in env_partitions[:-1]]\n",
    "test_partition = ToxicityDataset(env_partitions[-1][['id', 'toxicity', 'comment_text']], transform=t)[:]\n",
    "\n",
    "print(train_envs[0]['x'].shape, test_partition['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(train_envs, open('tenvs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(ref)\n",
    "\n",
    "args = {'lr': 0.0001, \\\n",
    "         'n_iterations':70000, \\\n",
    "         'penalty_anneal_iters':1, \\\n",
    "         'l2_reg':1.0, \\\n",
    "         'pen_wgt':1000, \\\n",
    "         'hid_layers':1, \\\n",
    "         'verbose':False}\n",
    "base = ref.LinearInvariantRiskMinimization('cls')\n",
    "model, errors, penalties, losses = base.train(train_envs, 1000, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logits = base.predict(np.concatenate([train_envs[i]['x'] for i in range(len(train_envs))]), model)\n",
    "train_labels = np.concatenate([train_envs[i]['y'] for i in range(len(train_envs))])\n",
    "test_logits = base.predict(test_partition['x'], model)\n",
    "test_labels = test_partition['y']\n",
    "\n",
    "train_acc = ref.compute_loss(np.expand_dims(train_logits, axis=1), train_labels, ltype='ACC')\n",
    "test_acc = ref.compute_loss(np.expand_dims(test_logits, axis=1), test_labels, ltype='ACC')\n",
    "print('train score: {}'.format(train_acc))\n",
    "print('test score: {}'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('none.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "559px",
    "left": "1542px",
    "right": "20px",
    "top": "115px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
